1
00:00:34,370 --> 00:00:38,020
&gt;&gt; So to give this answer, David again uses background knowledge.

2
00:00:38,020 --> 00:00:42,230
Someone else in the class might say that, well, he does not think that

3
00:00:42,230 --> 00:00:47,490
this particular wedge here is the same kind of block as the brick. And

4
00:00:47,490 --> 00:00:51,780
therefore this is not an example of foo. So I want to draw a number of

5
00:00:51,780 --> 00:00:57,730
lessons from this particular exercise. First, learning is often incremental.

6
00:00:57,730 --> 00:01:04,140
We learn from one example at a time. Cognitive agents, human beings,

7
00:01:04,140 --> 00:01:09,280
intelligent agents in general, are not always given hundreds or thousands or

8
00:01:09,280 --> 00:01:14,600
millions of examples right from the beginning. We get one example at a time.

9
00:01:14,600 --> 00:01:17,880
Second, often the examples that we get, are labeled.

10
00:01:17,880 --> 00:01:22,830
There is a teacher which tells us this a positive example, or this is a negative

11
00:01:22,830 --> 00:01:26,790
example. This is called supervised learning in machine learning literature.

12
00:01:26,790 --> 00:01:30,600
Because here there is a teacher which has labeled all the examples for

13
00:01:30,600 --> 00:01:34,990
you. Third, the examples can come in a particular order.

14
00:01:34,990 --> 00:01:39,030
There are always some positive examples, always some negative examples.

15
00:01:39,030 --> 00:01:43,340
The first example, typically is a positive example. Fourth,

16
00:01:43,340 --> 00:01:46,890
this is quite different from case based reasoning. In case based reasoning,

17
00:01:46,890 --> 00:01:51,880
which we had discussed last time, we had all of these examples, which we

18
00:01:51,880 --> 00:01:57,620
stored in their raw form in memory. We'll reuse them. I this particular case,

19
00:01:57,620 --> 00:02:03,010
however, we're abstracting concepts from there. Fifth, the number

20
00:02:03,010 --> 00:02:07,100
of examples from which we're extracting concepts is very small. We're not

21
00:02:07,100 --> 00:02:10,810
talking here about millions of examples from which we're doing the abstraction.

22
00:02:10,810 --> 00:02:15,530
Sixth, when we are trying to abstract concepts from examples, then,

23
00:02:15,530 --> 00:02:19,720
what exactly to abstract? What exactly, to learn? What exactly to generalize,

24
00:02:19,720 --> 00:02:25,251
becomes a very hard problem. There is a tendency to often overgeneralize, or

25
00:02:25,251 --> 00:02:29,930
often, to overspecialize. How does the intelligent agent find out,

26
00:02:29,930 --> 00:02:33,950
exactly what is [UNKNOWN] generalization? These are hard questions, and

27
00:02:33,950 --> 00:02:35,675
we'll look at some of these questions in just a minute.
